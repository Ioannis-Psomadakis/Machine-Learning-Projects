{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "52dcd79e",
   "metadata": {},
   "source": [
    "# Convolution Neural Networks"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4346752e",
   "metadata": {},
   "source": [
    "For this project the images from the MNIST database will be used. MNIST contains images of handwritten digits from 0 to 9 and includes 60,000 grayscale training images of size 28*28 pixels and 10,000 test images of the same dimensions and features(K=10 classes). It also contains labels for all images.\n",
    "Solving the MNIST database problem involves partitioning the data into classes in a way that all the contents of each cluster belong to the same class. More specifically, each cluster and, by extension, class, will consist of images containing a handwritten digit from 0 to 9. A Convolution Neural Network(CNN) will be train so that it can separate the data and, given an input-image, can \"recognize ” the class in which it belongs(that is, to identify the number included in the image)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f96aa29b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Installation of the libraries keras και tensorflow\n",
    "!pip install keras\n",
    "!pip install tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "66207045",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import of the necessary libraries and the MNIST dataset\n",
    "from keras import layers\n",
    "from keras import models\n",
    "from keras.datasets import mnist\n",
    "from keras.utils import to_categorical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "12bcd4fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading the dataset from the keras library datasets by importing mnist\n",
    "(train_images, train_labels), (test_images, test_labels) = mnist.load_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0747aa4",
   "metadata": {},
   "source": [
    "### Preprocessing of the data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f24a3535",
   "metadata": {},
   "source": [
    "Defining of the dimensions of each image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "963ba0c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The input is 256 28*28 pixels gray-scale pictures\n",
    "# 60000 instances for training and 10000 for testing\n",
    "train_images = train_images.reshape((60000, 28, 28, 1))\n",
    "test_images = test_images.reshape((10000, 28, 28, 1))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95164ddf",
   "metadata": {},
   "source": [
    "It is more common to use 32-bit precision when training a neural network, so at this point the training data should be converted to 32-bit floats by dividing by 255. As for dividing by 255, this is the maximum value of a byte, so this will ensure that the input features scale between 0.0 and 1.0. This is not mandatory, but it is usually preferable to have input attributes on this scale."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b265a54c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setting the type of train_images and test_images to float32 dividing by 255\n",
    "train_images = train_images.astype('float32')/255\n",
    "test_images = test_images.astype('float32')/255\n",
    "train_labels = to_categorical(train_labels)\n",
    "test_labels = to_categorical(test_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f8898dcf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Implementation of the CNN model\n",
    "model = models.Sequential()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63fb2c11",
   "metadata": {},
   "source": [
    "### Layer creation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6fa4842",
   "metadata": {},
   "source": [
    "Defining of the dimensions of the filter to be used and the dimension of the pooling window for sub-sampling the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "33285e55",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using the Conv2D layer from the keras library with 32 neurons in the first layer and with a filter size of 3*3\n",
    "model.add(layers.Conv2D(32, (3, 3), activation = 'relu', padding = 'same', input_shape = (28, 28, 1)))\n",
    "model.add(layers.MaxPooling2D((2, 2)))\n",
    "model.add(layers.Conv2D(64, (3, 3), activation = 'relu', padding = 'valid'))\n",
    "model.add(layers.MaxPooling2D((2, 2)))\n",
    "model.add(layers.Conv2D(64, (3, 3), activation = 'relu'))\n",
    "\n",
    "# The first Conv2D layer has a padding = same, while the second has a padding = valid\n",
    "# (padding = valid is equivalent to no padding\n",
    "#padding = same means that the size of the output of this layer must be equal to the size of the input)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93069a8b",
   "metadata": {},
   "source": [
    "In total, the implementation of padding, convolutional layer with ReLU as the activation function and a max-pooling layer is being used"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8e498dd3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " conv2d (Conv2D)             (None, 28, 28, 32)        320       \n",
      "                                                                 \n",
      " max_pooling2d (MaxPooling2D  (None, 14, 14, 32)       0         \n",
      " )                                                               \n",
      "                                                                 \n",
      " conv2d_1 (Conv2D)           (None, 12, 12, 64)        18496     \n",
      "                                                                 \n",
      " max_pooling2d_1 (MaxPooling  (None, 6, 6, 64)         0         \n",
      " 2D)                                                             \n",
      "                                                                 \n",
      " conv2d_2 (Conv2D)           (None, 4, 4, 64)          36928     \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 55,744\n",
      "Trainable params: 55,744\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Using the summary function, all layer names, parameters and shapes are displayed\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1176b442",
   "metadata": {},
   "source": [
    "Addition of a flattened layer along with two fully connected layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "063f0366",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The final fully connected layer should have enough neurons equal to the number of classes\n",
    "# we want to classify along with a SoftMax activation function\n",
    "model.add(layers.Flatten())\n",
    "model.add(layers.Dense(64, activation = 'relu'))\n",
    "model.add(layers.Dense(10, activation = 'softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "83fc294d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " conv2d (Conv2D)             (None, 28, 28, 32)        320       \n",
      "                                                                 \n",
      " max_pooling2d (MaxPooling2D  (None, 14, 14, 32)       0         \n",
      " )                                                               \n",
      "                                                                 \n",
      " conv2d_1 (Conv2D)           (None, 12, 12, 64)        18496     \n",
      "                                                                 \n",
      " max_pooling2d_1 (MaxPooling  (None, 6, 6, 64)         0         \n",
      " 2D)                                                             \n",
      "                                                                 \n",
      " conv2d_2 (Conv2D)           (None, 4, 4, 64)          36928     \n",
      "                                                                 \n",
      " flatten (Flatten)           (None, 1024)              0         \n",
      "                                                                 \n",
      " dense (Dense)               (None, 64)                65600     \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 10)                650       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 121,994\n",
      "Trainable params: 121,994\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Using of the summary function again\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43f29d15",
   "metadata": {},
   "source": [
    "### Compiling the model using the compile() function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e63af71c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use of the RMSprop optimizer, the Multi-Class Cross-Entropy Loss function\n",
    "# and the Accuracy metric to evaluate the model\n",
    "model.compile(optimizer = 'rmsprop', loss = 'categorical_crossentropy', metrics = ['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b744db3b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "938/938 [==============================] - 28s 28ms/step - loss: 0.1601 - accuracy: 0.9497\n",
      "Epoch 2/5\n",
      "938/938 [==============================] - 25s 27ms/step - loss: 0.0436 - accuracy: 0.9865\n",
      "Epoch 3/5\n",
      "938/938 [==============================] - 25s 27ms/step - loss: 0.0291 - accuracy: 0.9907\n",
      "Epoch 4/5\n",
      "938/938 [==============================] - 25s 27ms/step - loss: 0.0224 - accuracy: 0.9927\n",
      "Epoch 5/5\n",
      "938/938 [==============================] - 25s 27ms/step - loss: 0.0168 - accuracy: 0.9950\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1bb13d8ba60>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Feeding of the model and settting of the number of epochs(5 in this case)\n",
    "# Settting the sample size to 64\n",
    "model.fit(train_images, train_labels, epochs = 5, batch_size = 64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "42092652",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "313/313 [==============================] - 3s 8ms/step - loss: 0.0277 - accuracy: 0.9915\n"
     ]
    }
   ],
   "source": [
    "# Model check\n",
    "test_loss, test_acc = model.evaluate(test_images, test_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "6255171e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test accuracy: 0.9915000200271606\n"
     ]
    }
   ],
   "source": [
    "# Displaying the accuracy\n",
    "print('Test accuracy:', test_acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cbae7ec",
   "metadata": {},
   "source": [
    "Our model displayed an accuracy of 99.15%! An excellent level of accuracy, nearly similar to the levels of accuracy one would expect from a human."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "845cc63d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
